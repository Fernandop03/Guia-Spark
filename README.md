# Guia-Spark
-------------
# üöÄ Despliegue y Optimizaci√≥n de Cluster Spark en Docker Swarm

Este repositorio contiene la configuraci√≥n y las instrucciones detalladas para desplegar un cluster de **Apache Spark 3.5.5** sobre **Docker Swarm**. El enfoque principal es la **optimizaci√≥n de la asignaci√≥n de recursos** a los nodos worker de Spark, bas√°ndose en las especificaciones de hardware de cada m√°quina. Esto asegura la estabilidad del cluster, previene fallos por falta de recursos (OOM) y maximiza la utilizaci√≥n eficiente de tu infraestructura.

-----

### 1\. üí° Filosof√≠a de Arquitectura y Optimizaci√≥n

Un cluster Spark distribuido depende cr√≠ticamente de la correcta asignaci√≥n de recursos a sus componentes (Master y Workers). Una configuraci√≥n inadecuada puede llevar a:

  * **Fallas de Nodos Worker:** Si un worker solicita m√°s CPU o RAM de la que un nodo f√≠sico puede proporcionar (despu√©s de considerar los recursos para el sistema operativo y el propio Docker), el nodo puede volverse inestable, o el contenedor del worker se detendr√° o reiniciar√° constantemente.
  * **Subutilizaci√≥n de Recursos:** Asignar menos recursos de los disponibles desaprovecha el potencial de tus nodos m√°s potentes.

Para mitigar estos problemas, hemos implementado una estrategia de **Clasificaci√≥n de Nodos (Node Tiers)**. Esto implica agrupar tus m√°quinas por capacidades de hardware similares (CPU y RAM) y definir **perfiles de recursos Spark expl√≠citos** para cada grupo.

-----

### 2\. üìä Clasificaci√≥n de Nodos y Asignaci√≥n de Recursos Spark

Hemos analizado detalladamente los # nodos de tu infraestructura. La categorizaci√≥n y la asignaci√≥n de recursos se basan en un criterio conservador para dejar un margen de seguridad para el sistema operativo del host y el *overhead* de Docker (aproximadamente 1-2 n√∫cleos de CPU y 2-4 GB de RAM).

| Categor√≠a de Rendimiento | Nodos (Hostname)                                                                                                                                                                                                            | Procesador (Cores/Threads) | RAM (GB) | Rol Principal en Cluster | Recursos Spark Worker Asignados (`SPARK_WORKER_CORES`, `SPARK_WORKER_MEMORY`) |
| :----------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------- | :------- | :----------------------- | :----------------------------------------------------------------------------- |
| **Maestro (Master)** | `PC-YAGO`                                                                                                                                                                                                                   | i7-7700 (4C/8T)            | 16       | Spark Master, Jupyter    | N/A (No es worker de datos, se enfoca en orquestaci√≥n e interfaz)              |
| **Muy Alto (Very High)** | `PC-BRUNO`                                                                                                                                                                                                                  | Ryzen 5 3600 (6C/12T)      | 16       | Spark Worker             | **CPU: 5**, **RAM: 14G** (6C - 1C; 16GB - 2GB)                                  |
| **Alto (High)** | `PC-JESUS`, `PC-JORGE`                                                                                                                                                                                                      | i5-3470 / i7-7700 (4C/8T)  | 16       | Spark Worker             | **CPU: 3**, **RAM: 14G** (4C - 1C; 16GB - 2GB)                                  |
| **Medio (Mid)** | `PC-MIGUEL`, `PC-DANIEL`, `PC-ANGEL`, `PC-ELIAS`, `PC-CESAR`, `PC-CHAVA`, `PC-ROBERTO`, `PC-DAVID`, `PC-ALEX`, `PC-PAOLA`, `PC-ADRIAN`, `PC-ISAI`, `PC-ERIK`, `PC-LUCAS`, `PC-ULISES`, `PC-CARLOS` (16 nodos) | i5-3470 (4C)               | 8        | Spark Worker             | **CPU: 3**, **RAM: 6G** (4C - 1C; 8GB - 2GB)                                   |

-----

### 3\. ‚öôÔ∏è Archivos de Configuraci√≥n Esenciales

Estos archivos son la base de tu despliegue y deben residir en el directorio ra√≠z de este repositorio.

#### 3.1. `node_tiers.env`

Este archivo define los **nombres de host** para cada categor√≠a de rendimiento de nodos. Es **absolutamente crucial** que los nombres de host aqu√≠ listados coincidan **exactamente** con los que Docker Swarm reporta para tus m√°quinas. Puedes verificar los nombres de host en tu Swarm con `docker node ls`.

```bash
# Configuration for node performance tiers
# Add or remove hostnames from these lists as needed.
# Ensure hostnames match exactly what 'docker node ls' reports.

# Master node hostname
# IMPORTANT: Verify this matches the hostname reported by 'docker node ls' for your master node.
MASTER_HOSTNAME_CONFIG="PC-YAGO"

# Worker Node Performance Tiers

# VERY_HIGH_PERF_HOSTNAMES: Nodos con Ryzen 5 3600 y 16GB RAM
VERY_HIGH_PERF_HOSTNAMES=("PC-BRUNO")

# HIGH_PERF_HOSTNAMES: Nodos con i5-3470/i7-7700 y 16GB RAM
HIGH_PERF_HOSTNAMES=("PC-JESUS" "PC-JORGE")

# MID_PERF_HOSTNAMES: Nodos con i5-3470 y 8GB RAM (la mayor√≠a)
MID_PERF_HOSTNAMES=("PC-MIGUEL" "PC-DANIEL" "PC-ANGEL" "PC-ELIAS" "PC-CESAR" "PC-CHAVA" "PC-ROBERTO" "PC-DAVID" "PC-ALEX" "PC-PAOLA" "PC-ADRIAN" "PC-ISAI" "PC-ERIK" "PC-LUCAS" "PC-ULISES" "PC-CARLOS")

# LOW_PERF_HOSTNAMES: Not used in this configuration.
LOW_PERF_HOSTNAMES=()
```

#### 3.2. `setup_swarm.sh`

Este script es responsable de preparar tu Docker Swarm. Crea la red *overlay* necesaria para la comunicaci√≥n entre contenedores y, fundamentalmente, **aplica las etiquetas `spark-role` y `performance`** a cada uno de tus nodos. Estas etiquetas son la base para que Docker Swarm pueda desplegar los servicios Spark Worker en los nodos con las capacidades correctas.

```bash
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# --- General Configuration ---
# Name of the overlay network that Spark services will use.
# Must match the network name defined in 'docker-compose.yml'.
NETWORK_NAME="hadoop_spark_net"

# --- Node Tier Configuration ---
# This file contains the hostname definitions for each performance tier.
CONFIG_FILE="node_tiers.env"

# Verify if the configuration file exists.
if [[ -f "$CONFIG_FILE" ]]; then
    echo "Loading node tier configuration from $CONFIG_FILE..."
    # 'source' loads variables defined in the file into the current script's environment.
    source "$CONFIG_FILE"
else
    echo "ERROR: Configuration file '$CONFIG_FILE' not found."
    echo "Please ensure '$CONFIG_FILE' exists and contains the necessary definitions."
    exit 1
fi

# Validate that the master node's primary variable is defined.
: "${MASTER_HOSTNAME_CONFIG:?ERROR: MASTER_HOSTNAME_CONFIG is not defined in $CONFIG_FILE. Please define it.}"

echo "----------------------------------------------------"
echo "Preparing Docker Swarm infrastructure..."
echo "----------------------------------------------------"

# 1. Create or Verify the Overlay Network
echo "Checking/Creating overlay network '$NETWORK_NAME'..."
if ! docker network ls | grep -q "$NETWORK_NAME"; then
    docker network create -d overlay --attachable "$NETWORK_NAME"
    echo "  -> Network '$NETWORK_NAME' created successfully."
else
    echo "  -> Network '$NETWORK_NAME' already exists. Continuing."
fi
echo "----------------------------------------------------"

# 2. Labeling Swarm Nodes
echo "Starting node labeling process for the Swarm..."
# Iterate over each node in the Swarm to apply the correct labels.
docker node ls --format "{{.ID}}\t{{.Hostname}}" | while read NODE_ID NODE_HOSTNAME; do
    echo "  Processing node: '$NODE_HOSTNAME' (ID: '$NODE_ID')"

    # Clean up previous Spark role and performance labels to ensure a clean configuration.
    echo "    -> Clearing previous Spark and performance labels (if they exist)..."
    docker node update --label-rm spark-role --label-rm performance "$NODE_ID" 2>/dev/null || true # '2>/dev/null || true' to ignore errors if the label doesn't exist.

    SPARK_ROLE_LABEL=""       # Variable to store the Spark role (master/worker)
    PERFORMANCE_LABEL=""      # Variable to store the performance level (very-high/high/mid/low)

    # Assign master role to the node configured as MASTER_HOSTNAME_CONFIG.
    if [[ "$NODE_HOSTNAME" == "$MASTER_HOSTNAME_CONFIG" ]]; then
        SPARK_ROLE_LABEL="spark-role=master"
        echo "    -> Assigning role: '$SPARK_ROLE_LABEL'"
        docker node update --label-add "$SPARK_ROLE_LABEL" "$NODE_ID"
    fi

    # Assign worker role and performance level to the other nodes.
    # The master node will NOT be labeled as a worker in this configuration to dedicate its resources to control.
    if [[ "$NODE_HOSTNAME" != "$MASTER_HOSTNAME_CONFIG" ]]; then
        SPARK_ROLE_LABEL="spark-role=worker"
        echo "    -> Assigning role: '$SPARK_ROLE_LABEL'"
        docker node update --label-add "$SPARK_ROLE_LABEL" "$NODE_ID"

        # Search and assign the performance level (searched in order from most to least powerful).
        # Check Very High Performance list
        for hostname_in_list in "${VERY_HIGH_PERF_HOSTNAMES[@]}"; do
            if [[ "$hostname_in_list" == "$NODE_HOSTNAME" ]]; then
                PERFORMANCE_LABEL="performance=very-high"
                break
            fi
        done

        # Check High Performance list
        if [[ -z "$PERFORMANCE_LABEL" ]]; then # Only if a performance label hasn't been found yet
            for hostname_in_list in "${HIGH_PERF_HOSTNAMES[@]}"; do
                if [[ "$hostname_in_list" == "$NODE_HOSTNAME" ]]; then
                    PERFORMANCE_LABEL="performance=high"
                    break
                fi
            done
        fi

        # Check Mid Performance list
        if [[ -z "$PERFORMANCE_LABEL" ]]; then
            for hostname_in_list in "${MID_PERF_HOSTNAMES[@]}"; do
                if [[ "$hostname_in_list" == "$NODE_HOSTNAME" ]]; then
                    PERFORMANCE_LABEL="performance=mid"
                    break
                fi
            done
        fi

        # Check Low Performance list
        if [[ -z "$PERFORMANCE_LABEL" ]]; then
            for hostname_in_list in "${LOW_PERF_HOSTNAMES[@]}"; do
                if [[ "$hostname_in_list" == "$NODE_HOSTNAME" ]]; then
                    PERFORMANCE_LABEL="performance=low"
                    break
                fi
            done
        fi

        # Apply the performance label if one was found.
        if [[ -n "$PERFORMANCE_LABEL" ]]; then
            docker node update --label-add "$PERFORMANCE_LABEL" "$NODE_ID"
            echo "    -> Assigned performance level: '$PERFORMANCE_LABEL'"
        else
            echo "    -> WARNING: No specific performance level defined for hostname '$NODE_HOSTNAME'."
            echo "       This node will not be used by Spark Worker services that require a performance label."
        fi
    fi
    echo "  ----------------------------------------------------"
done

echo ""
echo "Node labeling process completed."
echo "----------------------------------------------------"
echo "VERIFICATION:"
echo "To confirm that labels were applied correctly, run:"
echo "  docker node ls --format 'table {{.Hostname}}\t{{.Status}}\t{{.Labels}}'"
echo "Or for a specific node:"
echo "  docker node inspect <node_id_or_hostname> | grep -A 5 Labels"
echo "----------------------------------------------------"
echo "Script 'setup_swarm.sh' finished!"
```

#### 3.3. `docker-compose.yml`

Este archivo define todos los servicios Docker que formar√°n tu cluster Spark. Se incluye un nuevo servicio para los workers de "Muy Alto Rendimiento" y se ajustan las asignaciones de CPU (`SPARK_WORKER_CORES`), RAM (`SPARK_WORKER_MEMORY`) y el n√∫mero de `replicas` para cada tipo de worker seg√∫n la clasificaci√≥n anterior.

```yaml
version: '3.8'

services:
  spark-master:
    image: bitnami/spark:3.5.5
    hostname: spark-master
    user: root # Run as root to avoid permission issues in some environments.
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_LOCAL_IP=0.0.0.0
      - SPARK_DRIVER_HOST=spark-master
      - SPARK_DRIVER_BIND_ADDRESS=0.0.0.0
      - SPARK_USER=root
      - SPARK_RPC_AUTHENTICATION_ENABLED=no # Simplified for test/dev environment.
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8080:8080" # Spark Master UI
      - "7077:7077" # Spark Master RPC Port
    deploy:
      mode: replicated
      replicas: 1 # A single instance of the Master.
      placement:
        constraints:
          - node.labels.spark-role == master # Deploys on the node labeled as master.
    networks:
      - hadoop_spark_net # Overlay network for inter-service communication.

  # Service for VERY HIGH PERFORMANCE Nodes (e.g., PC-BRUNO)
  spark-worker-very-high:
    image: bitnami/spark:3.5.5
    hostname: "spark-worker-vhigh-{{.Node.Hostname}}" # Dynamic hostname for identification.
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077 # Points to the Spark master.
      - SPARK_WORKER_WEBUI_PORT=8081 # Worker UI port.
      - SPARK_WORKER_CORES=5 # CPU allocation: 5 cores for Spark.
      - SPARK_WORKER_MEMORY=14G # RAM allocation: 14 GB for Spark.
    ports:
      - "8081:8081" # Exposes the worker UI port.
    deploy:
      mode: replicated
      replicas: 1 # There is 1 Very High Performance node.
      placement:
        constraints:
          - node.labels.spark-role == worker
          - node.labels.performance == very-high # Deploys on nodes with this label.
    networks:
      - hadoop_spark_net

  # Service for HIGH PERFORMANCE Nodes (e.g., PC-JESUS, PC-JORGE)
  spark-worker-high:
    image: bitnami/spark:3.5.5
    hostname: "spark-worker-high-{{.Node.Hostname}}"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_WEBUI_PORT=8083 # Different port to avoid conflicts with other worker types if they were on the same host (though this setup avoids it).
      - SPARK_WORKER_CORES=3 # CPU allocation: 3 cores for Spark.
      - SPARK_WORKER_MEMORY=14G # RAM allocation: 14 GB for Spark.
    ports:
      - "8083:8083"
    deploy:
      mode: replicated
      replicas: 2 # There are 2 High Performance nodes.
      placement:
        constraints:
          - node.labels.spark-role == worker
          - node.labels.performance == high
    networks:
      - hadoop_spark_net

  # Service for MID PERFORMANCE Nodes (the majority of your nodes)
  spark-worker-mid:
    image: bitnami/spark:3.5.5
    hostname: "spark-worker-mid-{{.Node.Hostname}}"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_WEBUI_PORT=8084 # Another different port.
      - SPARK_WORKER_CORES=3 # CPU allocation: 3 cores for Spark.
      - SPARK_WORKER_MEMORY=6G # RAM allocation: 6 GB for Spark.
    ports:
      - "8084:8084"
    deploy:
      mode: replicated
      replicas: 16 # There are 16 Mid Performance nodes.
      placement:
        constraints:
          - node.labels.spark-role == worker
          - node.labels.performance == mid
    networks:
      - hadoop_spark_net

  jupyter-notebook:
    image: jupyter/pyspark-notebook:latest
    hostname: jupyter-notebook
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_HOME=/usr/local/spark
      - JUPYTER_TOKEN=sparkcluster # Jupyter Lab access token (!!!CHANGE IN PRODUCTION!!!)
    ports:
      - "8888:8888" # Jupyter Lab access port.
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.spark-role == master # Deploys on the master node.
    networks:
      - hadoop_spark_net

networks:
  hadoop_spark_net:
    external: true # The network must be created externally by the setup_swarm.sh script.
```

#### 3.4. `dhcp-compose.yml`

Este archivo contiene el servicio DHCP. Es crucial levantarlo **antes** que cualquier otro servicio del cluster para asegurar que los nodos reciban sus IPs correctamente en tu red aislada (si aplica).

```yaml
# Before bringing up this service, ensure you assign a static IP to your network interface connected to the switch.
#
# 1. Verify the connection name with:
#    nmcli connection show
#
#    Example output:
#    NAME                UUID                                  TYPE      DEVICE
#    Wired connection 1  1c2b8...                              ethernet  enx68e43b307d75
#
# 2. Assign a manual IP by running (adjust the connection name if necessary):
#    sudo nmcli connection modify "Wired connection 1" \
#      ipv4.method manual \
#      ipv4.addresses 192.168.0.2/24 \
#      ipv4.gateway 192.168.0.1 \
#      ipv4.dns 8.8.8.8
#
# 3. Restart the connection to apply changes:
#    sudo nmcli connection down "Wired connection 1"
#    sudo nmcli connection up "Wired connection 1"
#
# Once done, you can bring up this DHCP service with Docker Compose.


version: "3.8"

services:
  dhcp:
    image: jpillora/dnsmasq
    container_name: dhcp
    restart: unless-stopped
    network_mode: "host"
    cap_add:
      - NET_ADMIN
      - NET_RAW
    command:
      - "-k"
      - "--port=0"
      - "--log-facility=-"
      - "--interface=enx68e43b..." # !!!IMPORTANT!!! Change this to your actual network interface name (e.g., eth0, enp0s3, enx68e43b307d75)
      - "--dhcp-range=192.168.0.100,192.168.0.200,1h" # IP range to assign and lease time.
      - "--dhcp-option=option:router,192.168.0.1" # Gateway (your router).
      - "--dhcp-option=option:dns-server,8.8.8.8,8.8.4.4" # DNS servers.
```

-----

### 4\. üöÄ Flujo de Despliegue Detallado

Sigue estos pasos en el orden indicado para un despliegue exitoso de tu cluster Spark.

#### 4.1. **Pre-despliegue (En cada nodo: Maestro y Workers)**

1.  **Instalar Docker y Docker Compose:**
      * Aseg√∫rate de que Docker Engine y Docker Compose est√©n instalados y configurados en **todos tus 20 nodos**. Consulta la documentaci√≥n oficial de Docker para tu distribuci√≥n de Linux.
2.  **Configurar Carpetas de Datos (En cada nodo Worker):**
      * Para asegurar que los workers Spark tengan espacio para datos temporales o *shuffle*, crea una carpeta con permisos adecuados en **cada nodo worker** (todos excepto `PC-YAGO`):
        ```bash
        sudo mkdir -p /data/spark
        sudo chmod 777 /data/spark # Permisos de escritura para el contenedor
        ```

#### 4.2. **Configuraci√≥n y Arranque del Servicio DHCP (Solo en el Nodo Maestro: `PC-YAGO`)**

Este paso es **fundamental** para la asignaci√≥n de IPs en tu red. El servicio DHCP debe estar operativo antes de que los nodos worker se unan o intenten obtener IPs.

1.  **Asignar IP Est√°tica al Maestro:**
      * Con√©ctate a tu nodo maestro (`PC-YAGO`).
      * **Identifica tu Interfaz de Red:** Ejecuta `nmcli connection show` y anota el nombre de tu conexi√≥n de red cableada (ej., `Wired connection 1`, `enp0s3`, `eth0`).
      * **Configura la IP Est√°tica:** Sustituye `YOUR_NETWORK_INTERFACE_NAME` por el nombre real de tu interfaz y ajusta la IP, gateway y DNS seg√∫n tu configuraci√≥n de red.
        ```bash
        sudo nmcli connection modify "YOUR_NETWORK_INTERFACE_NAME" \
          ipv4.method manual \
          ipv4.addresses 192.168.0.2/24 \
          ipv4.gateway 192.168.0.1 \
          ipv4.dns "8.8.8.8,1.1.1.1"
        ```
      * **Reinicia la Conexi√≥n:**
        ```bash
        sudo nmcli connection down "YOUR_NETWORK_INTERFACE_NAME"
        sudo nmcli connection up "YOUR_NETWORK_INTERFACE_NAME"
        ```
      * **Verifica la IP:** Confirma que la interfaz tiene la IP est√°tica asignada:
        ```bash
        ip a show YOUR_NETWORK_INTERFACE_NAME
        ```
2.  **Preparar y Ejecutar `dhcp-compose.yml`:**
      * **¬°Edita `dhcp-compose.yml`\!** Cambia la l√≠nea `--interface=enx68e43b...` para usar el nombre real de tu interfaz de red en el nodo maestro.
      * Guarda el archivo `dhcp-compose.yml` en tu nodo maestro.
      * Levanta el servicio DHCP:
        ```bash
        docker compose -f dhcp-compose.yml up -d
        ```
      * **Verificaci√≥n:** Aseg√∫rate de que el contenedor `dhcp` est√© en estado `Up`:
        ```bash
        docker ps
        ```

#### 4.3. **Inicializaci√≥n y Configuraci√≥n de Docker Swarm (Desde el Nodo Maestro)**

Todos los siguientes comandos se ejecutan desde el nodo maestro (`PC-YAGO`).

1.  **Inicializar Swarm (si no est√° inicializado):**

      * Si tu Swarm no est√° activo, inicial√≠zalo:
        ```bash
        docker swarm init --advertise-addr <IP_ESTATICA_DEL_MAESTRO>
        ```
      * **¬°Anote el token\!** Este comando le proporcionar√° un token de uni√≥n. Lo necesitar√° para el siguiente paso.

2.  **Unir Nodos Workers al Swarm (En cada nodo Worker):**

      * Con√©ctate a **cada uno de tus 19 nodos worker** (PC-MIGUEL, PC-DANIEL, etc.).
      * Ejecuta el comando `docker swarm join` que obtuviste del maestro:
        ```bash
        docker swarm join --token <TOKEN_DEL_MAESTRO> <IP_ESTATICA_DEL_MAESTRO>:2377
        ```

3.  **Verificar Nodos en el Swarm (Desde el Maestro):**

      * Aseg√∫rate de que todos los 20 nodos est√©n listados y en estado `Ready`:
        ```bash
        docker node ls
        ```

4.  **Copiar y Ejecutar `setup_swarm.sh`:**

      * Copia los archivos **actualizados** `node_tiers.env` y `setup_swarm.sh` a la misma carpeta en el nodo maestro.
      * Haz el script ejecutable y l√°nzalo:
        ```bash
        chmod +x setup_swarm.sh
        ./setup_swarm.sh
        ```
      * **Verificar Etiquetas (¬°MUY IMPORTANTE\!):** Despu√©s de que el script termine, valida que las etiquetas `spark-role` y `performance` se aplicaron correctamente:
        ```bash
        docker node ls --format 'table {{.Hostname}}\t{{.Status}}\t{{.Labels}}'
        ```
        `PC-YAGO` debe tener `spark-role=master`. Los otros nodos deben tener `spark-role=worker` y su respectiva etiqueta de rendimiento (`performance=very-high`, `performance=high`, o `performance=mid`).

#### 4.4. **Despliegue del Cluster Spark (Desde el Nodo Maestro)**

1.  **Copia el `docker-compose.yml`:**

      * Aseg√∫rate de tener la **versi√≥n m√°s reciente** del `docker-compose.yml` en la misma carpeta desde donde ejecutar√°s el comando.

2.  **Desplegar el Stack de Spark:**

    ```bash
    docker stack deploy -c docker-compose.yml spark-cluster
    ```

    Puedes elegir un nombre diferente para tu stack si lo deseas (ej., `my-spark-app`).

3.  **Verificar Servicios Desplegados:**

      * Confirma que todos los servicios se han desplegado con el n√∫mero de r√©plicas esperado:
        ```bash
        docker service ls
        ```
        Deber√≠as ver 1 r√©plica para `spark-master` y `jupyter-notebook`, 1 para `spark-worker-very-high`, 2 para `spark-worker-high` y 16 para `spark-worker-mid`.

4.  **Monitorear Logs de los Workers:**

      * Si alg√∫n worker no se inicia o falla, revisa sus logs para diagnosticar el problema:
        ```bash
        docker service logs spark-cluster_spark-worker-mid --follow # Ejemplo para workers mid
        # O para otros tipos:
        # docker service logs spark-cluster_spark-worker-high --follow
        # docker service logs spark-cluster_spark-worker-very-high --follow
        ```

#### 4.5. **Acceso a las Interfaces Web**

Una vez que todos los servicios est√©n `Running`, puedes acceder a las UIs desde tu navegador:

  * **Spark Master UI:** `http://<IP_ESTATICA_DEL_MAESTRO>:8080`
  * **Jupyter Lab:** `http://<IP_ESTATICA_DEL_MAESTRO>:8888/lab` (usa el token `sparkcluster`. **¬°CAMBIAR EN PRODUCCI√ìN POR SEGURIDAD\!**)

-----

### 5\. üõ†Ô∏è Previsiones, Resoluci√≥n de Problemas y Mantenimiento

Esta secci√≥n te equipa con el conocimiento para diagnosticar y resolver los problemas m√°s comunes, y mantener tu cluster saludable.

#### 5.1. **Problemas Comunes y Diagn√≥stico**

  * **Problema: Un nodo no se etiqueta correctamente o tiene etiquetas duplicadas.**

      * **S√≠ntoma:** Los servicios Spark Worker no se despliegan en los nodos correctos, o `docker service ls` muestra menos r√©plicas de las esperadas.
      * **Diagn√≥stico:**
          * Verifica las etiquetas de tus nodos: `docker node ls --format 'table {{.Hostname}}\t{{.Status}}\t{{.Labels}}'`
          * Confirma que el `hostname` del nodo en `node_tiers.env` coincide **exactamente** con el que `docker node ls` reporta.
      * **Soluci√≥n:** Si hay etiquetas incorrectas, elim√≠nalas manualmente (`docker node update --label-rm <nombre_etiqueta> <node_id_o_hostname>`) y luego vuelve a ejecutar `./setup_swarm.sh`.

  * **Problema: Un worker Spark no se inicia o "muere" (el contenedor se reinicia constantemente).**

      * **S√≠ntoma:** `docker ps` muestra el contenedor en `Exited` o `restarting`. `docker service ls` muestra `0/N` r√©plicas o un n√∫mero menor al esperado.
      * **Diagn√≥stico (¬°CR√çTICO\!):** **Revisa los logs del servicio afectado.** Usa `docker service logs spark-cluster_<nombre_del_servicio_worker> --follow`.
          * **Errores de Memoria (OOM):** Si ves `OutOfMemoryError`, "JVM exited" o "heap space", `SPARK_WORKER_MEMORY` es demasiado alto para la RAM disponible del nodo.
          * **Problemas de Conectividad:** Si hay mensajes como "failed to connect to master" o "connection refused", revisa la red `hadoop_spark_net` y la accesibilidad del puerto `7077` en el maestro.
      * **Soluci√≥n:**
        1.  **Ajuste de Recursos:** Si es un problema de memoria/CPU, **reduce** los valores de `SPARK_WORKER_CORES` o `SPARK_WORKER_MEMORY` en el `docker-compose.yml` para el tipo de worker afectado. Considera dejar un margen m√°s grande para el SO y Docker (ej., 3-4GB de RAM).
        2.  **Re-desplegar:** Despu√©s de cualquier cambio en `docker-compose.yml`, siempre actualiza el stack: `docker stack deploy -c docker-compose.yml spark-cluster`.

  * **Problema: Un nodo f√≠sico se cae o se desconecta del Swarm.**

      * **Previsi√≥n:** Docker Swarm es inherentemente resiliente. Intentar√° autom√°ticamente reprogramar las r√©plicas de los servicios de ese nodo en otros nodos disponibles que cumplan con los mismos `placement constraints` (etiquetas).
      * **Monitoreo:** `docker service ls` mostrar√° el n√∫mero actual de r√©plicas (ej., `1/2` si una r√©plica de dos fall√≥). Una vez que el nodo se recupere y se una al Swarm, las r√©plicas se balancear√°n de nuevo.

  * **Problema: Discrepancias en el conteo de nodos (m√°s o menos nodos de los esperados con workers).**

      * **Diagn√≥stico:** Compara el n√∫mero de hostnames en cada lista de `node_tiers.env` con el n√∫mero de `replicas` configurado en `docker-compose.yml` y con la salida real de `docker node ls`.
      * **Soluci√≥n:**
          * **Nodo Extra:** Si tienes un nodo que no deber√≠a ser worker Spark, pero lo est√° siendo, aseg√∫rate de que **no est√©** en ninguna de las listas `_PERF_HOSTNAMES` en `node_tiers.env`. Si ya tiene etiquetas, elim√≠nalas manualmente (`docker node update --label-rm spark-role --label-rm performance <node_id>`) y ejecuta `docker stack deploy` de nuevo.
          * **Nodo Faltante:** Si un nodo esperado no tiene workers o no est√° etiquetado, verifica su estado (`docker node ls`, `docker node inspect <hostname>`). Aseg√∫rate de que est√© encendido, unido al Swarm y que su hostname est√© correctamente listado en `node_tiers.env`.

#### 5.2. **Mejores Pr√°cticas y Consideraciones Futuras**

  * **Actualizaciones:** Para actualizar las im√°genes de Spark o Jupyter, solo necesitas modificar el tag de la imagen en `docker-compose.yml` y ejecutar `docker stack deploy`. Docker Swarm gestionar√° la actualizaci√≥n gradual.
  * **Persistencia:** Para cargas de trabajo que requieren persistencia de datos (aunque Spark procesa datos transitorios en RAM), considera el uso de vol√∫menes persistentes.
  * **Seguridad:** Este setup est√° simplificado para un ambiente de desarrollo/prueba. Para producci√≥n, es **IMPRESCINDIBLE** configurar la autenticaci√≥n, encriptaci√≥n y SSL para Spark y Jupyter, as√≠ como implementar reglas de firewall robustas.
  * **Escalabilidad:** Si necesitas escalar, simplemente agrega m√°s nodos con recursos similares a un tier existente, actualiza `node_tiers.env`, ejecuta `setup_swarm.sh` y luego incrementa las `replicas` correspondientes en `docker-compose.yml` antes de desplegar de nuevo.

-----

¬°Con esta gu√≠a, tienes una base s√≥lida y optimizada para operar tu cluster Spark \!
